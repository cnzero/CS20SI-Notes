{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage Experiments\n",
    "- More word2vec\n",
    "- tf.train.Saver\n",
    "- tf.summary\n",
    "- Randomization\n",
    "- Data Readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "x = tf.Variable(2.0)\n",
    "y = 2.0 * (x**3)\n",
    "z = 3.0 + y**2\n",
    "\n",
    "grad_z = tf.gradients(z, [x,y])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    writer = tf.summary.FileWriter('tmp/CS20SI5/', sess.graph)\n",
    "    print(sess.run(grad_z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need models to be reusable\n",
    "\n",
    "- Q: How do we make our model easy to reuse?\n",
    "- A: take advantage of Python's OOP - Object Oriented Programming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SkipGramModel:\n",
    "    def __init__(self, params):\n",
    "        pass\n",
    "    def _create_placeholder(self):\n",
    "        '''Step 1: define the placeholder for input and output'''\n",
    "        pass\n",
    "    def _create_embedding(self):\n",
    "        '''Step 2: define weights. In word2vec, it is actually the weights that we care about. '''\n",
    "        pass\n",
    "    \n",
    "    def _create_loss(self):\n",
    "        '''Step 3 + 4: define the inference + the loss function'''\n",
    "        pass\n",
    "    \n",
    "    def _create_optimizer(self):\n",
    "        '''Step 5: define optimizer'''\n",
    "        pass\n",
    "    \n",
    "    def _create_summaries(self):\n",
    "        with tf.name_scope('summaries'):\n",
    "            tf.summary.scalar('loss', sef.loss)\n",
    "            tf.summary.scalar('accuracy', self.accuracy)\n",
    "            tf.summary.histogram('histogram loss', self.loss)\n",
    "            # because you have several summaries, we should merge them all into one op to make it easier to manage. \n",
    "            self.summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save sessions, not graphs\n",
    "`tf.train.Saver.save(sess, save_path, global_step=None...)`\n",
    "- save parameters after N steps, each saved step is a checkpoint\n",
    "- Restore variables\n",
    "\n",
    "为了避免在模型训练过程中出现一些意外崩溃的情况，或者主动停掉训练过程，避免之前的训练工作（深度学习等训练时间一般较长）徒劳，最好能进行一段时间的训练后就对模型参数进行一定的保存。\n",
    "\n",
    "如下，每训练1000步进行参数保存：\n",
    "\n",
    "首先定义 `global_step`\n",
    "\n",
    "```\n",
    "self.global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "# update global_step during training\n",
    "self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss, global_step = self.global_step)\n",
    "```\n",
    "\n",
    "在训练过程中记录 `global_step` 对应下的模型参数，并保存到指定的断点文件。\n",
    "```\n",
    "# define model\n",
    "\n",
    "# create a saver object\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# launch a session to compute the graph\n",
    "with tf.Session() as sess:\n",
    "    # actual training loop\n",
    "    for step in range(training_steps):\n",
    "        sess.run([optimizer])\n",
    "        \n",
    "        if (step + 1) % 1000 == 0:\n",
    "            saver.save(sess, 'checkpoint_directory/model_name', global_step = model.global_step)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过运行程序，就能够在指定文件夹下看到保存的断点文件，如下所示：\n",
    "\n",
    "![CheckpointsFiles](CheckpointsFiles.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那既然保存了断点下的模型训练参数，就肯定有朝一日会重新拿回来再用，或继续训练等，用如下方式进行取回\n",
    "\n",
    "```\n",
    "saver.restore(sess, 'checkpoints/skip-gram-10000')\n",
    "```\n",
    "\n",
    "或者稍作优化下，通过程序检查最新的断点文件，如果存在就取回，然后继续进行模型训练过程； 如果不存在就从最开始进行训练，并且在训练过程中不断保存断点文件， 这样可以避免 _低水平重复劳动_ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "默认情况下，`saver.save()` 会存储计算图中所有的变量。\n",
    "\n",
    "但是仍然可以通过自定义 list or dict 的方式，指定存储的变量。\n",
    "\n",
    "```\n",
    "# variables that are used in the computational graph\n",
    "v1 = tf.Variable(..., name='v1')\n",
    "v2 = tf.Variable(..., name='v2')\n",
    "\n",
    "# - method one: pass variables as a dict:\n",
    "saver = tf.train.Saver({'v1':v1, 'v2':v2})\n",
    "\n",
    "# - method two: pass variables as a list:\n",
    "saver = tf.train.Saver([v1, v2])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Attentions:__\n",
    "\n",
    "每次存储或取回的都是变量及其断点下的取值，并不包含整个计算图computational graph，所以在需要时还得重新搭建Model。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize our summary statistics during our training\n",
    "\n",
    "相对于原来使用 `matplotlib` 绘制变量在训练过程中变化情况， __TensorBoard__ 提供了更加完善的可视化功能，应当善加利用。\n",
    "- Step 1: create summaries\n",
    "- Step 2: run them\n",
    "- Step 3: write summaris to file\n",
    "- Step 4: see summaries on TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "def _create_summaries(self):\n",
    "    with tf.name_scope('summaries'):\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        tf.summary.scalar('accuracy', self.accuracy)\n",
    "        tf.summary.histogram('histogram loss', self.loss)\n",
    "        tf.summary.image(name, tensor, max_outputs=3, collections=None)\n",
    "        self.summary_op = tf.summary.merge_all()\n",
    "```\n",
    "像是别的变量一样需要对该`summary_op` 进行运行：\n",
    "\n",
    "```\n",
    "loss_batch, _, summary = sess.run([model.loss, model.optimizer, model.summary_op], feed_dict = feed_dict)\n",
    "```\n",
    "并且写到文件里面去：\n",
    "```\n",
    "writer.add_summary(summary, global_step = step)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为深度学习模型中确实有许多参数需要调节， TensorBoard也提供了可视化的效果用于参数比较。如 `learning_rate` 为0.5或1.0，分别保存到文件夹`improved_graph/lr1.0`和 `impropved_graph/lr0.5`，然后两者就能够进行直接比较，如下图所示：\n",
    "\n",
    "![Learning Rate selection](LR1.JPG) ![Learning Rate Coomparison](LR2.JPG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Control Randomization\n",
    "何时会需要Randomization\n",
    "- 初始化权重矩阵；\n",
    "- 对训练样本进行随机排序形成batch；\n",
    "\n",
    "但是就会出现一个问题，那如果需要复现某些模型较好的结果，这些随机化的东西如何进行控制呢？\n",
    "总不能模型“随机”出现了一个好结果，但是以后就无法复现了，这好像并不能说服人。\n",
    "- Operations level random seed\n",
    "- Sessions keep track of random state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.06166208]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# a = tf.Variable(tf.truncated_normal( (-1.0, 1.0), stddev=0.1, seed=0) )\n",
    "a = tf.truncated_normal([1], seed=3)\n",
    "with tf.Session() as sess:\n",
    "    print(sess.run(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Readers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
